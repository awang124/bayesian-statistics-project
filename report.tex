\documentclass[notitlepage, 12pt]{report}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{setspace}
\usepackage[letterpaper, portrait, margin=1.5in]{geometry}

\title{\vspace{-1.5cm} Spatial Gaussian Process Regression for \\ Bayesian Optical Flow Estimation}
\author{Andy Wang \& Luca Orquiza}
\date{December 2025}
\begin{document}
\maketitle

\newcommand{\lb}{\left[}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\rb}{\right]}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\var}{\text{Var}}
\newcommand{\mle}{\text{MLE}}
\newcommand{\bm}{\boldsymbol{\mu}}
\newcommand{\be}{\boldsymbol{\eta}}
\renewcommand{\vec}{\boldsymbol}
\renewcommand{\thesection}{\arabic{section}}

\begin{abstract}
Classical computer vision methods have long been used to estimate optical flow fields for image sequences. These methods produce deterministic point estimates, failing to capture uncertainty inherent in optical flow estimation. We propose a probabilistic variation of the Lucas-Kanade method, which returns initial observed flows and error covariance estimates. We discuss spatial Gaussian process regression (GPR) to specify prior probability distributions on latent optical flow fields conditioned on these observations. We describe our method, expose GPR's statistical formulation and parameter fitting via empirical Bayes, and apply GPR to estimate optical flows for a benchmark image sequence.
\end{abstract}

\section{Introduction}
\hspace{\parindent} Optical flow fields are an invaluable interdisciplinary tool, used throughout computer vision, robotics, human-computer interaction, medical imaging, and scientific research to perform object detection, motion segmentation, object tracking, action recognition, video stabilization, video compression, image registration, etc. A motion field is a 3-dimensional vector field that describes the velocity of objects moving in space: an optical flow field is a projection of such a motion field onto a 2-dimensional plane, viewed alternatively as a 2D vector field expressing the motion occurring between two frames in a sequence of images at each pixel. Optical flow estimation is the inverse problem of estimating this field given only a pair of images. \\

As a natural consequence of optical flows' widespread use, accurate optical flow estimation has been an active subject of research for decades. Notably, (Lucas \& Kanade, 1981) introduced local smoothness by assuming equal motion for all pixels in image windows, and computing flows via least-squares estimation. (Horn \& Schunck, 1981) estimated optical flow by minimizing a global energy functional, with a smoothness regularization term, via calculus of variations. Modern deep learning approaches include FlowNet (Dosovitskiy et al., 2015) and Recurrent All-Pairs Field Transforms (RAFT) (Teed \& Deng, 2020), which employ a convolutional neural network and recurrent neural network, respectively. \\

These methods are all deterministic in nature, producing point (exact) optical flow estimates without any measure of confidence or uncertainty in their predictions. Uncertainty is inherent in optical flow estimation, due to image noise, brightness changes, lack of texture, object occlusion, the aperture problem, and multiple incompatible motions in localized regions. Furthermore, quantification of confidence is paramount in applications where erroneous estimates might have catastropic consequences, such as autonomous navigation, computer-integrated surgery, and real-time surveillance. As such, significant efforts have been made to make optical flow estimation probabilistic in nature: (Simoncelli et al., 1991) model brightness constraint errors with Gaussian noise and derive a posterior flow distribution, (Roy \& Govindu, 2000) formulate optical flow as a Markov Random Field labeling problem and solve it via graph cuts on angle and magnitude parameters, and (Wannenwestch et al., 2017) perform variational inference on an energy-based model using mean-field approximation to predict optical flow and uncertainty (entropy of the variational distribution). \\

We propose a novel method for obtaining maximum-likelihood based optical flow estimates, a probabilistic variation of the Lucas-Kanade method that incorporates additive Gaussian noise. We then discuss Gaussian process regression, a nonparameteric Bayesian method, which uses our ML estimates as its likelihood. We assume a Gaussian process prior over our latent ``flow-generating function", meaning our flows flow a multivariate Gaussian distribution determined by a chosen mean and covariance kernel. The kernel hyperparameters, and thus the latent mean and covariance, are optimized by maximizing the marginal likelihood of the observations via gradient descent. After optimization, the posterior mean and covariance provide each pixel's optical flow estimate and associated uncertainty, respectively. We apply this method on the Yosemite image sequence (Barron et al., 1994), a well-known optical flow benchmark dataset consisting of 15 synthetic images, ``captured" by a drone flying over Yosemite National Park. Finally, we discuss our results and potential directions for improvement.

\section{Methodology}
\subsection{Wang-Orquiza Method}
Consider latent vector $\vec{x}\in\mathbb{R}^n$, and observed vector $\vec{y}\in\mathbb{R}^n$. Assume $\vec{y}=\vec{A}\vec{x}+\vec{\eta}$, for some fixed $\vec{A}\in\mathbb{R}^{m\times n}$, and additive Gaussian noise $\vec{\eta}\in\mathbb{R}^n\sim\mathcal{N}(\vec{0},\vec{H})$. We may compute a maximum-likelihood estimate for $\vec{x}$ based on $\vec{y}$, where the likelihood we maximize is that of $\vec{\eta}$. Namely: \[\hat{\vec{x}}_{\mle}(\vec{y})=\arg\max_{\vec{x}}f_{\vec{y}}(\vec{y}|\vec{x})=\arg\max_{\vec{x}}f_{\vec{\eta}}(\vec{y}-\vec{A}\vec{x})=\vec{\Sigma}_{\hat{\vec{x}}}\vec{A}^T\vec{H}^{-1}\vec{y}\] Here we used error covariance matrix: \[\vec{\Sigma}_{\hat{\vec{x}}}=\cov[\hat{\vec{x}}_{\mle}(\vec{y})-\vec{x}]=(\vec{A}^T\vec{H}^{-1}\vec{A})^{-1}\] We use this result to propose a reformulation of the Lucas-Kanade equation, where we include additive Gaussian noise: \[\vec{I_t}=-\vec{\nabla I}^T\vec{f}+\vec{\eta}\qquad\vec{\eta}\sim\mathcal{N}(\vec{0},\vec{H})\] Here $\vec{\nabla I}\in\mathbb{R}^{2\times n}$ is our horizontal and vertical image derivatives, and $\vec{I}_t\in\mathbb{R}^{n}$ is our temporal derivatives, and $\vec{f}\in\mathbb{R}^2$ is our flow vector, for all pixels in a window. Our maximum-likelihood method then gives us our estimated optical flow vector, and its error covariance matrix, for each pixel: \[\tilde{\vec{f}}=-\vec{\Sigma}_{\tilde{\vec{f}}}\vec{\nabla I}\vec{H}^{-1}\vec{I}_t\in\mathbb{R}^2\qquad\vec{\Sigma}_{\tilde{\vec{f}}}=(\vec{\nabla I}\vec{H}^{-1}\vec{\nabla I}^T)^{-1}\in\mathbb{R}^{2\times2}\] We see if an object at pixel $\vec{x}$ has low texture (meaning small image brightness change from that pixel to its neighbors), it will have small gradient, $\vec{\nabla}I$, which in turn implies a large error covariance $\vec{\Sigma}_{\tilde{\vec{f}}}(\vec{x})$ at $\vec{x}$. \\\\ (We've excluded a detailed exposition of optical flow computation for brevity, since it doesn't pertain to statistics: see (Lucas \& Kanade, 1981) for details. All that's important is that our primary innovation with this method is the production of an error covariance $\vec{\Sigma}_{\tilde{\vec{f}}}$, which quantifies prediction confidence).

\newpage
\subsection{Spatial Gaussian Processes}
A 2-dimensional \textit{spatial Gaussian process} is a distribution of functions with 2-dimensional outputs: \[\vec{f}(\cdot)=\begin{bmatrix}u(\cdot)\\v(\cdot)\end{bmatrix}\sim\mathcal{GP}(\vec{m}(\cdot),\vec{k}(\cdot,\cdot))\] Such a function evaluated at any finite subset of points follows a joint Gaussian distribution: \[\begin{bmatrix}\vec{f}(\vec{x}_1)\\\vdots\\\vec{f}(\vec{x}_n)\end{bmatrix}\sim\mathcal{N}\lp\begin{bmatrix}\vec{m}(\vec{x}_1)\\\ddots\\\vec{m}(\vec{x}_n)\end{bmatrix},\begin{bmatrix}\vec{k}(\vec{x}_1,\vec{x}_1)&\cdots&\vec{k}(\vec{x}_1,\vec{x}_n)\\\vdots&\ddots&\vdots\\\vec{k}(\vec{x}_n,\vec{x}_1)&\cdots&\vec{k}(\vec{x}_n,\vec{x}_n)\end{bmatrix}\rp\] The precise distribution is determined by mean kernel $\vec{m}$ and covariance kernel $\vec{k}$, the choice of which encodes the prior over the process (Álvarez et al., 2012). Common $\vec{m}$ include zero, constant, and affine kernels: \[\vec{m}(\cdot)=\vec{0}\qquad\vec{m}(\cdot)=\vec{c}\qquad\vec{m}(\cdot)=\vec{A}\vec{x}+\vec{b}\] Common $\vec{k}$ include Gaussian ($L^2$) and Laplace ($L^1$) kernels: \[\vec{k}_{L^2}(\cdot,\cdot')=\exp\lp-\frac{\|\cdot-\cdot'\|^2}{2\lambda^2}\rp\vec{\Sigma}\qquad\vec{k}_{L^1}(\cdot,\cdot')=\exp\lp-\frac{\|\cdot-\cdot'\|}{\lambda}\rp\vec{\Sigma}\] We chose a constant mean and Gaussian covariance, encoding a constant drift and loss of correlation as flows grow farther apart. Note that while kernel formulations are user-chosen, kernel hyper parameters are optimized via empirical Bayes. An equivalent joint distribution formulation neatly separating horizontal, vertical, and cross covariances is: \[\vec{F}\coloneq\begin{bmatrix}\vec{U}(\vec{X})\\\vec{V}(\vec{X})\end{bmatrix}\sim\mathcal{N}\lp\bm\coloneq\begin{bmatrix}\bm_u\\\bm_v\end{bmatrix},\vec{K}\coloneq\begin{bmatrix}\vec{K}_{uu}&\vec{K}_{uv}\\\vec{K}_{vu}&\vec{K}_{vv}\end{bmatrix}\rp\] $\vec{X}=\{\vec{x}_i\}_{i=1}^N$, $\vec{U}(\vec{X})_i=u(\vec{x}_i)$, $\vec{V}(\vec{X})_i=v(\vec{x}_i)$, $(\bm_u)_i=\vec{m}(\vec{x}_i)_1$, \mbox{$(\bm_v)_i=\vec{m}(\vec{x}_i)_2$} \\\\ \mbox{$\vec{\tilde{k}}=\vec{k}(\vec{x}_i,\vec{x}_j)\implies$ $(\vec{K}_{uu})_{ij}=\vec{\tilde{k}}_{11}$, $(\vec{K}_{uv})_{ij}=\vec{\tilde{k}}_{12}$, $(\vec{K}_{vu})_{ij}=\vec{\tilde{k}}_{21}$, $(\vec{K}_{vu})_{ij}=\vec{\tilde{k}}_{22}$} \\\\ Here, $\vec{F}\in\mathbb{R}^{2N}$, where $N$ is the number of pixels, is the true flow field to be estimated, with concatenated horizontal and vertical components.

\subsection{Gaussian Process Regression}
We assume each Wang-Orquiza observation is its corresponding true flow plus some independent additive Gaussian noise: \[\vec{\tilde{f}}(\vec{x})=\vec{f}(\vec{x})+\vec{\be}(\vec{x}),\quad\vec{\be}(\vec{x})\sim\mathcal{N}\lp\vec{0},\vec{\Sigma}_{\tilde{\vec{f}}}(\vec{x})\rp\] We want to determine marginal distribution $\vec{\tilde{F}}\in\mathbb{R}^{2N}$ via: \[p(\vec{\tilde{F}})=\int p(\vec{\tilde{F}}|\vec{F})p(\vec{F})\,d\vec{F}\] The convolution of two Gaussians is Gaussian (Rasmussen \& Williams, 2006). \[\vec{\tilde{F}}|\vec{F}\sim\mathcal{N}\lp\vec{F},\vec{\Sigma}_{\tilde{\vec{f}}}\rp\qquad\vec{F}\sim\mathcal{N}\lp\bm,\vec{K}\rp\] We use these to compute the mean and variance of $\vec{\tilde{F}}$: \[\ex[\vec{\tilde{F}}]=\ex[\ex[\vec{\tilde{F}}|\vec{F}]]=\ex[\vec{F}]=\bm\] \[\var(\vec{\tilde{F}})=\var(\ex[\vec{\tilde{F}}|\vec{F}])+\ex[\var(\vec{\tilde{F}}|\vec{F})]=\var(\vec{F})+\ex[\vec{\Sigma}_{\tilde{\vec{f}}}]=\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\] Thus $\vec{\tilde{F}}\sim\mathcal{N}\lp\bm,\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp$. See Appendix A for \mbox{a more through proof. Also:} \[\cov(\vec{F},\vec{\tilde{F}})=\cov(\vec{F},\vec{F})+\cov(\vec{F},\vec{\Sigma}_{\tilde{\vec{f}}})=\vec{K}+\vec{0}=\vec{K}\] All this allows us to formulate the joint distribution of $(\vec{F},\vec{\tilde{F}})$: \[\begin{bmatrix}\vec{F}\\\vec{\tilde{F}}\end{bmatrix}\sim\mathcal{N}\lp\begin{bmatrix}\bm\\\bm\end{bmatrix},\begin{bmatrix}\vec{K}&\vec{K}\\\vec{K}&\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\end{bmatrix}\rp\] Finally, we derive the posterior distribution $\vec{F}|\vec{\tilde{F}}$ \mbox{(Rasmussen \& Williams, 2006):} \[\vec{F}|\vec{\tilde{F}}\sim\mathcal{N}\lp\bm+\vec{K}\lp\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp^{-1}\lp\vec{\tilde{F}}-\bm\rp,\vec{K}-\vec{K}\lp\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp^{-1}\vec{K}\rp\] See Appendix B for a full derivation. This posterior gives us our updated optical flow estimates and uncertainties. It is computed after kernel hyperparameters are optimized via empirical Bayes (Rasmussen \& Williams, 2006) using gradient descent. See Appendix C for mathematical details.

\section{Implementation}
The images below are the first two images of the Yosemite sequence. Motion is minute, perhaps most visible at the images' lower left corners: \begin{center}\includegraphics[totalheight=4.5cm]{img/yosemite.png}\end{center} The images are $252\times316$ RGB images, which we first grayscaled. Spatial and temporal derivatives were respectively computed using Sobel filters and forward differencing. Lucas-Kanade and Wang-Orquiza computation was performed using $15\times15$ windows. GPR was performed using PyTorch, with a constant mean and RBF covariance kernel. Gradient descent was performed using Adam optimization (Kingma \& Ba, 2014) (learning rate $\eta=0.1$).

\section{Results}
Below is our Lucas-Kanade optical flow field, as a baseline. We see more turbulent estimates in the sky (top) and cliff (bottom-left) sections, which correspond to their low texture in the image pair. Contrast this with the high-texture valley (bottom-right), which already has much smoother estimates. We see our motivation for a quantification of confidence.
\begin{center}\includegraphics[totalheight=5.5cm]{img/lucas-kanade.png}\end{center}

Yosemite images reproduced for convenience:
\begin{center}\includegraphics[totalheight=4.5cm]{img/yosemite.png}\end{center}
Below is our Wang-Orquiza optical flow field (left), along with our error covariance visualization (right). Each pixel has a $(2\times2)$ covariance matrix, and we plot its larger eigenvalue (log-scle), which represents that pixel's maximal variance over all directions. We confirm out intuition, and our model is more uncertain in estimating flows for the sky and cliff regions.
\begin{center}\includegraphics[totalheight=3.5cm]{img/wang-orquiza.png}\end{center}
Below are our GPR posterior mean (left) and covariance eigenvalues (right). Our estimates and uncertainties have both effectively been denoised, and our resultant smoothed flows and variances, due to our Gaussian prior, are much more realistic.
\begin{center}\includegraphics[totalheight=3.5cm]{img/gaussian-process.png}\end{center}

\section{Discussion}
\hspace{\parindent} Our novel optical flow estimation methodology combining our Wang-Orquiza reformulation or Lucas-Kanade with spatial Gaussian process regression proved effective in enforcing spatial smoothness between optical flows and producing estimates of prediction confidence. There are a myriad of directions in which to extend this work, and we list the most interesting of these below. \\

\begin{itemize}
\item Identify if certain kernel formulations better suit certain image sequences, through theoretical exploration or empirical comparison. For example, when is Gaussian or Laplace covariance preferred over the other?
\item Obtain better observations, likely via deep learning (e.g. FlowNet, RAFT).
\item Perform semantic/instance segmentation, then perform GPR on each segment independently, to capture different motions in localized regions.
\item Determine if a 3-dimensional Gaussian process can model sequences of optical flow fields, capturing spatial and temporal flow correlations.
\end{itemize}

Optical flow fields are a vastly valuable tool across many applications. This work has demonstrated the potential for probabilistic optical flow computation through Gaussian processes, to offer a small contribution to this fascinating intersection of computer vision and Bayesian statistics.

\newpage
\begin{spacing}{0.8}
\section*{References}
Álvarez, M. A., Rosasco, L., \& Lawrence, N. D. (2012). Kernels for vector-valued functions: A review. \textit{Foundations and Trends in Machine Learning, 4}(3), 195-266. \\\\
Barron, J. L., Fleet, D. J., \& Beauchemin, S. S. (1994). Performance of Optical Flow Techniques. \textit{International Journal of Computer Vision, 12}(1), 43-77. \\\\
Dosovitskiy A., Fischer P., Ilg E., Häusser P., Hazirbaş C., \& Golkov V. (2015). FlowNet: Learning Optical Flow with Convolutional Networks. \textit{Proceedings of the IEEE International Conference on Computer Vision}, 2758-2766. \\\\
Horn, B. K. P., \& Schunck, B. G. (1981). Determining Optical Flow. \textit{Artificial Intelligence, 17}(1-3), 185-203. \\\\
Kingma, D. P., \& Ba, J. (2014). Adam: A Method for Stochastic Optimization. \textit{3rd International Conference for Learning Representations}. \\\\
Lucas, B. D., \& Kanade, T. (1981). An Iterative Image Registration Technique with an Application to Stereo Vision. \textit{7th International Joint Conference on Artificial Intelligence, 2}, 674-679. \\\\
Magnus, J. R., \& Neudecker, H. (1999). Matrix Differential Calculus with Applications in Statistics and Econometrics. Wiley. \\\\
Micheli, M. (2025). Mathematical Image Analysis [Unpublished lecture notes]. Johns Hopkins University. \\\\
Rasmussen, C. E., \& Williams, C. K. I. (2006). Gaussian Process for Machine Learning. MIT Press. \\\\
Roy, S., \& Govindu, V. (2000). MRF Solutions for Probabilistic Optical Flow Formulations. \textit{Proceedings 15th International Conference on Pattern Recognition, 3}, 1041-1047. \\\\
Simoncelli, E. P., Adelson, E. H., \& Heeger, D. J. (1991). Probability Distributions of Optical Flow. \textit{CVPR, 91}, 310-315. \\\\
Teed, Z., \& Deng, J. (2020). RAFT: Recurrent All-Pairs Field Transforms for Optical Flow. \textit{Computer Vision--ECCV 2020: 16th European Conference, 2}(16), 402-419. \\\\
Wannenwetsch, A. S., Keuper, M., \& Roth, S. (2017). ProbFlow: Joint Optical Flow and Uncertainty Estimation. \textit{Proceedings of the IEEE International Conference on Computer Vision}, 1173-1182. \\\\
Zhang, F. (2005). The Schur complement and its applications. Springer Science \& Business Media.
\end{spacing}

\section*{Appendix A}
Here we prove $\vec{\tilde{F}}|\vec{F}\sim\mathcal{N}\lp\vec{F},\vec{\Sigma}_{\tilde{\vec{f}}}\rp$ and $\vec{F}\sim\mathcal{N}\lp\bm,\vec{K}\rp$ implies $\vec{\tilde{F}}\sim\mathcal{N}\lp\bm,\vec{K}\rp$. For simplicity of derivations, we assume $\vec{\Sigma}_{\tilde{\vec{f}}}=\sigma_{\be}^2\vec{I}$: \[\footnotesize \begin{aligned} p\lp\vec{\tilde{F}}\rp &= \int p\lp\vec{\tilde{F}}|\vec{F}\rp p\lp\vec{F}\rp\,d\vec{F} \\& \propto\int\exp\lp-\frac{1}{2}\lp\vec{\tilde{F}}-\vec{F}\rp^T\frac{1}{\sigma_{\be}^2}\vec{I}\lp\vec{\tilde{F}}-\vec{F}\rp\rp\exp\lp-\frac{1}{2}\lp\vec{F}-\bm\rp^T\vec{K}^{-1}\lp\vec{F}-\bm\rp\rp\,d\vec{F} \\&= \int\exp\lp-\frac{1}{2}\lb\frac{1}{\sigma_{\be}^2}\vec{F}^T\vec{F}-\frac{2}{\sigma_{\be}^2}\vec{\tilde{F}}^T\vec{F}+\frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}^T\vec{\tilde{F}}+\vec{F}^T\vec{K}^{-1}\vec{F}-2\bm^T\vec{K}^{-1}\vec{F}+\bm^T\vec{K}^{-1}\bm\rb\rp\,d\vec{F} \end{aligned}\] We are integrating over $\vec{F}$ and briefly group all terms not involving $\vec{F}$ \mbox{into a constant:} \[\footnotesize p\lp\vec{\tilde{F}}\rp\propto\int\exp\lp-\frac{1}{2}\lb\vec{F}^T\lp\frac{1}{\sigma_{\be}^2}\vec{I}+\vec{K}^{-1}\rp\vec{F}-2\lp\frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}^T+\bm^T\vec{K}^{-1}\rp\vec{F}+\text{const.}\rb\rp\,d\vec{F}\] Let $\vec{A}\coloneq\frac{1}{\sigma_{\be}^2}\vec{I}+\vec{K}^{-1},\vec{b}^T\coloneq\frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}^T-\bm^T\vec{K}^{-1}$. We complete the square: \[\footnotesize \vec{F}^T\vec{AF}-2\vec{b}^T\vec{F}=\lp\vec{F}-\vec{A}^{-1}\vec{b}\rp^T\vec{A}\lp\vec{F}-\vec{A}^{-1}\vec{b}\rp-\vec{b}^T\vec{A}^{-1}\vec{b}\] This yields: \[\footnotesize p\lp\vec{\tilde{F}}\rp\propto\exp\lp-\frac{1}{2}\lp\vec{F}-\vec{A}^{-1}\vec{b}\rp^T\vec{A}\lp\vec{F}-\vec{A}^{-1}\vec{b}\rp\rp\exp\lp\frac{1}{2}\lb\vec{b}^T\vec{A}^{-1}\vec{b}-\frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}^T\vec{\tilde{F}}-\bm^T\vec{K}^{-1}\bm\rb\rp\,d\vec{F}\] The first term integrates to the inverse of the Gaussian normalizing factor. Meanwhile, the second term does not depend on $\vec{F}$. Reincorporating the initial normalizing factor, we obtain: \[\footnotesize p\lp\vec{\tilde{F}}\rp=\frac{1}{\lp2\pi\sigma_{\be}\rp^{2N}\sqrt{\det\lp\vec{K}\rp}}\sqrt{\lp2\pi\rp^{2N}\det\lp\vec{A}^{-1}\rp}\exp\lp\frac{1}{2}\lb\vec{b}^T\vec{A}^{-1}\vec{b}-\frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}^T\vec{\tilde{F}}-\bm^T\vec{K}^{-1}\bm\rb\rp\] We use the intermediate result: \[\footnotesize \vec{A}=\vec{K}^{-1}+\frac{1}{\sigma_{\be}^2}\vec{I}=\vec{K}^{-1}\lp\vec{I}+\frac{1}{\sigma_{\be}^2}\vec{K}\rp=\frac{1}{\sigma_{\be}^2}\vec{K}^{-1}\lp\vec{K}+\sigma_{\be}^2\vec{I}\rp\] to compute $\det\lp\vec{A}^{-1}\rp$: \[\footnotesize \det\lp\vec{A}\rp^{-\frac{1}{2}}=\lp\frac{1}{\sigma_{\be}^{2n}}\rp^{-\frac{1}{2}}\det\lp\vec{K}^{-1}\rp^{-\frac{1}{2}}\det\lp\vec{K}+\sigma_{\be}^2\vec{I}\rp^{-\frac{1}{2}}=\sigma_{\be}^n\det\lp\vec{K}\rp^{\frac{1}{2}}\det\lp\vec{K}+\sigma_{\be}^2\vec{I}\rp^{-\frac{1}{2}}\] Substituting this into the above expression yields: \[\footnotesize p\lp\vec{\tilde{F}}\rp=\frac{1}{\sqrt{2\pi\det\lp\vec{K}+\sigma_{\be}^2\vec{I}\rp}}\exp\lp\frac{1}{2}\lb\vec{b}^T\vec{A}^{-1}\vec{b}-\frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}^T\vec{\tilde{F}}-\bm^T\vec{K}^{-1}\bm\rb\rp\] We express $\vec{b}^T\vec{A}^{-1}\vec{b}$ in terms of $\vec{\tilde{F}},\bm,\vec{K}$. Let $\vec{K}_{\sigma}\coloneq\vec{K}+\sigma_{\be}^2\vec{I}$: \[\begin{aligned} \vec{b}^T\vec{A}^{-1}\vec{b} &= \lp\frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}-\vec{K}^{-1}\bm\rp^T\lp\vec{I}+\frac{1}{\sigma_{\be}^2}\vec{K}\rp^{-1}\vec{K}\lp\frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}+\vec{K}^{-1}\bm\rp \\&= \sigma_{\be}^2\lp\frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}+\vec{K}^{-1}\bm\rp^T\vec{K}_{\sigma}^{-1}\vec{K}\lp\frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}+\vec{K}^{-1}\bm\rp \\&= \sigma_{\be}^2\lb\frac{1}{\sigma_{\be}^4}\vec{\tilde{F}}^T\vec{K}_{\sigma}^{-1}\vec{K}\vec{\tilde{F}}+\frac{2}{\sigma_{\be}^2}\vec{\tilde{F}}^T\vec{K}_{\sigma}^{-1}\bm+\bm^T\vec{K}^{-1}\vec{K}_{\sigma}^{-1}\bm\rb \\&= \frac{1}{\sigma_{\be}^2}\vec{\tilde{F}}^T\vec{\tilde{F}}-\vec{\tilde{F}}^T\vec{K}_{\sigma}^{-1}\vec{\tilde{F}}+2\vec{\tilde{F}}^T\vec{K}_{\sigma}^{-1}\bm+\sigma_{\be}^2\bm^T\vec{K}^{-1}\vec{K}_{\sigma}^{-1}\bm \end{aligned}\] Using this, the previous expression becomes: \[\begin{aligned} p\lp\vec{\tilde{F}}\rp & \propto\exp\lp\frac{1}{2}\lb-\vec{\tilde{F}}^T\vec{K}_{\sigma}^{-1}\vec{\tilde{F}}+2\vec{\tilde{F}}\vec{K}_{\sigma}^{-1}\bm+\bm^T\lp\sigma_{\eta}^2\vec{K}^{-1}\vec{K}_{\sigma}^{-1}-\vec{K}^{-1}\rp\bm\rb\rp \\&= \exp\lp\frac{1}{2}\lb-\vec{\tilde{F}}^T\vec{K}_{\sigma}^{-1}\vec{\tilde{F}}+2\vec{\tilde{F}}^T\vec{K}_{\sigma}^{-1}\bm+\text{const.}\rb\rp \end{aligned}\] We again complete the square: \[-\vec{\tilde{F}}^T\vec{K}_{\sigma}^{-1}\vec{\tilde{F}}+2\vec{\tilde{F}}\vec{K}_{\sigma}^{-1}\bm=-\lp\vec{\tilde{F}}-\bm\rp^T\vec{K}_{\sigma}^{-1}\lp\vec{\tilde{F}}-\bm\rp+\bm^T\vec{K}_{\sigma}^{-1}\bm\] This yields: \[\footnotesize p\lp\vec{\tilde{F}}\rp\propto\exp\lp-\frac{1}{2}\lp\vec{\tilde{F}}-\bm\rp^T\vec{K}_{\sigma}^{-1}\lp\vec{\tilde{F}}-\bm\rp\rp\exp\lp\frac{1}{2}\bm^T\lb\vec{K}_{\sigma}^{-1}+\sigma_{\eta}^2\vec{K}^{-1}\vec{K}_{\sigma}^{-1}-\vec{K}^{-1}\rb\bm\rp\] The middle term in the second exponent is: \[\vec{K}_{\sigma}^{-1}+\sigma_{\eta}^2\vec{K}^{-1}\vec{K}_{\sigma}^{-1}-\vec{K}^{-1}=\lp\vec{I}+\sigma_{\be}^2\vec{K}^{-1}\rp\vec{K}_{\sigma}^{-1}-\vec{K}^{-1}=\vec{K}^{-1}\vec{K}_{\sigma}\vec{K}_{\sigma}^{-1}-\vec{K}^{-1}=\vec{O}\] Thus we achieve the final expression for the density: \[p\lp\vec{\tilde{F}}\rp=\frac{1}{\sqrt{2\pi\det\lp\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp}}\exp\lp-\frac{1}{2}\lp\vec{\tilde{F}}-\bm\rp^T\lp\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp^{-1}\lp\vec{\tilde{F}}-\bm\rp\rp\] and subsequently the target distribution: \[\vec{\tilde{F}}\sim\mathcal{N}\lp\bm,\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp\]

\newpage
\section*{Appendix B}
Here we prove that if $\vec{X}_1,\vec{X}_2$ are multivariate normal, i.e.: \[\begin{bmatrix}\vec{X}_1\\\vec{X}_2\end{bmatrix}\sim\mathcal{N}\lp\begin{bmatrix}\bm_1\\\bm_2\end{bmatrix},\begin{bmatrix}\vec{\Sigma}_{11}&\vec{\Sigma}_{12}\\\vec{\Sigma}_{21}&\vec{\Sigma}_{22}\end{bmatrix}\rp\] then the conditional distribution of $\vec{X}_1|\vec{X}_2$ is: \[\vec{X}_1|\vec{X}_2\sim\mathcal{N}\lp\bm_1+\vec{\Sigma}_{12}\vec{\Sigma}_{22}^{-1}\lp\vec{X}_2-\bm_2\rp,\vec{\Sigma}_{11}-\vec{\Sigma}_{12}\vec{\Sigma}_{22}^{-1}\vec{\Sigma}_{21}\rp\] Conditioning on $\vec{X}_2$ means $\vec{X}_2$ is fixed. Thus: \[p_{\vec{X}_1|\vec{X}_2}(\vec{x}_1|\vec{x}_2)=\frac{p_{\vec{X}_1,\vec{X}_2}(\vec{x}_1,\vec{x}_2)}{p_{\vec{X}_2}(\vec{x}_2)}\propto p_{\vec{X}_1,\vec{X}_2}(\vec{x}_1,\vec{x}_2)\] Let $\vec{\tilde{\Sigma}}=\vec{\Sigma}^{-1}$. This density is proportional to: \[\exp\lp-\frac{1}{2}\begin{bmatrix}\vec{x}_1-\bm_1\\\vec{x}_2-\bm_2\end{bmatrix}^T\begin{bmatrix}\vec{\tilde{\Sigma}}_{11}&\vec{\tilde{\Sigma}}_{12}\\\vec{\tilde{\Sigma}}_{21}&\vec{\tilde{\Sigma}}_{22}\end{bmatrix}\begin{bmatrix}\vec{x}_1-\bm_1\\\vec{x}_2-\bm_2\end{bmatrix}\rp\] Notice that $\vec{\tilde{\Sigma}}_{ij}$ equals $\lp\vec{\Sigma}^{-1}\rp_{ij}$ and not $\lp\vec{\Sigma}_{ij}\rp^{-1}$. \mbox{Let $Q$ be the exponent. We expand:} \[\small Q\propto\lp\vec{x}_1-\bm_1\rp^T\vec{\tilde{\Sigma}}_{11}\lp\vec{x}_1-\bm_1\rp+2\lp\vec{x}_1-\bm_1\rp^T\vec{\tilde{\Sigma}}_{12}\lp\vec{x}_2-\bm_2\rp+\lp\vec{x}_2-\bm_2\rp^T\vec{\tilde{\Sigma}}_{22}\lp\vec{x}_2-\bm_2\rp\] We expand further and ignore all constant terms (those not involving $\vec{x}_1$): \[Q=\vec{x}_1^T\vec{\tilde{\Sigma}}_{11}\vec{x}_1-2\vec{x}_1^T\vec{\tilde{\Sigma}}_{11}\bm_1+2\vec{x}_1^T\vec{\tilde{\Sigma}}_{12}\lp\vec{x}_2-\bm_2\rp+\text{const.}\] We complete the square by finding $\bm_*,\vec{\Sigma}_*$: \[\lp\vec{x}_1-\bm_*\rp^T\vec{\Sigma}_*^{-1}\lp\vec{x}_1-\bm_*\rp=\vec{x}_1^T\vec{\Sigma}_*^{-1}\vec{x}_1-2\vec{x}_1\vec{\Sigma}_*^{-1}\bm_*=Q\] The quadratic terms must equate: \[\vec{x}_1^T\vec{\Sigma}_*^{-1}\vec{x}_1=\vec{x}_1^T\vec{\tilde{\Sigma}}_{11}\vec{x}_1\implies\vec{\Sigma}_*^{-1}=\vec{\tilde{\Sigma}}_{11}\implies\vec{\Sigma}_*=\vec{\tilde{\Sigma}}_{11}^{-1}\] The linear terms must also equate: \[-2\vec{x}_1\vec{\Sigma}_*^{-1}\bm_*=-2\vec{x}_1^T\vec{\tilde{\Sigma}}_{11}\bm_1+2\vec{x}_1^T\vec{\tilde{\Sigma}}_{12}\lp\vec{x}_2-\bm_2\rp\] Substituting $\vec{\Sigma}_*^{-1}=\vec{\tilde{\Sigma}}_{11}$ and simplifying yields: \[\vec{\tilde{\Sigma}}_{11}\bm_*=\vec{\tilde{\Sigma}}_{11}\bm_1-\vec{\tilde{\Sigma}}_{12}\lp\vec{x}_2-\bm_2\rp\implies\bm_*=\bm_1-\vec{\tilde{\Sigma}}_{11}^{-1}\vec{\tilde{\Sigma}}_{12}\lp\vec{x}_2-\bm_2\rp\] Thus, we have: \[p_{\vec{X}_1|\vec{X}_2}\propto\exp\lp-\frac{1}{2}\lp\vec{x}_1-\bm_*\rp^T\vec{\Sigma}_*\lp\vec{x}_1-\bm_*\rp\rp\implies\vec{X}_1|\vec{X}_2\sim\mathcal{N}\lp\bm_*,\vec{\Sigma}_*\rp\] We have absorbed all constants into the final density's normalizing factor. \\\\ All that remains is to determine $\vec{\tilde{\Sigma}}_{11},\vec{\tilde{\Sigma}}_{12}$. Zhang (2005) gives: \[\vec{M}=\begin{bmatrix}\vec{A}&\vec{B}\\\vec{C}&\vec{D}\end{bmatrix}=\begin{bmatrix}\vec{I}&\vec{BD}^{-1}\\\vec{O}&\vec{I}\end{bmatrix}\begin{bmatrix}\vec{S}&\vec{O}\\\vec{O}&\vec{D}\end{bmatrix}\begin{bmatrix}\vec{I}&\vec{O}\\\vec{D}^{-1}\vec{C}&\vec{I}\end{bmatrix}\] where Schur complement $\vec{S}\coloneq\vec{A}-\vec{BD}^{-1}\vec{C}$. We invert: \[\begin{aligned} \vec{M}^{-1} &= \begin{bmatrix}\vec{I}&-\vec{BD}^{-1}\\\vec{O}&\vec{I}\end{bmatrix}\begin{bmatrix}\vec{S}^{-1}&\vec{O}\\\vec{O}&\vec{D}^{-1}\end{bmatrix}\begin{bmatrix}\vec{I}&\vec{O}\\-\vec{D}^{-1}\vec{C}&\vec{I}\end{bmatrix} \\&= \begin{bmatrix}\vec{S}^{-1}&-\vec{S}^{-1}\vec{BD}^{-1}\\-\vec{D}^{-1}\vec{CS}^{-1}&\vec{D}^{-1}+\vec{D}^{-1}\vec{CS}^{-1}\vec{BD}^{-1}\end{bmatrix} \end{aligned}\] For $\vec{M}=\vec{\Sigma}$, Schur complement $\vec{\Sigma}_{S}\coloneq\vec{\Sigma}_{11}-\vec{\Sigma}_{12}\vec{\Sigma}_{22}^{-1}\vec{\Sigma}_{21}$, and: \[\vec{\tilde{\Sigma}}_{11}=\lp\vec{\Sigma}^{-1}\rp_{11}=\vec{\Sigma}_S^{-1}\qquad\vec{\tilde{\Sigma}}_{12}=\lp\vec{\Sigma}^{-1}\rp_{12}=-\vec{\Sigma}_S^{-1}\vec{\Sigma}_{12}\vec{\Sigma}_{22}^{-1}\] From these we derive the intermediate result: \[\vec{\tilde{\Sigma}}_{11}^{-1}\vec{\tilde{\Sigma}}_{12}=\vec{\Sigma}_S\lp-\vec{\Sigma}_S^{-1}\vec{\Sigma}_{12}\vec{\Sigma}_{22}^{-1}\rp=\vec{\Sigma}_{12}\vec{\Sigma}_{22}^{-1}\] These results yield expressions for $\bm_*,\vec{\Sigma}_*$ in terms of $\bm,\vec{\Sigma}$: \[\bm_*=\bm_1-\vec{\tilde{\Sigma}}_{11}^{-1}\vec{\tilde{\Sigma}}_{12}\lp\vec{x}_2-\bm_2\rp=\bm_1+\vec{\Sigma}_{12}\vec{\Sigma}_{22}^{-1}\lp\vec{x}_2-\bm_2\rp\] \[\vec{\Sigma}_*=\vec{\tilde{\Sigma}}_{11}^{-1}=\vec{\Sigma}_S=\vec{\Sigma}_{11}-\vec{\Sigma}_{12}\vec{\Sigma}_{22}^{-1}\vec{\Sigma}_{21}\] We have fully derived conditional distribution $\vec{X}_1|\vec{X}_2$. Substituting the appropriate mean and covariance parameters from the formulation of Gaussian process regression into this general expression yields \mbox{the posterior distribution:} \[\vec{F}|\vec{\tilde{F}}\sim\mathcal{N}\lp\bm+\vec{K}\lp\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp^{-1}\lp\vec{\tilde{F}}-\bm\rp,\vec{K}-\vec{K}\lp\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp^{-1}\vec{K}\rp\]

\section*{Appendix C}
Here we show how to maximize the marginal likelihood of $\vec{\tilde{F}}\sim\mathcal{N}\lp\bm,\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp$: \[p\lp\vec{\tilde{F}}\rp=\frac{1}{\sqrt{(2\pi)^{2N}\det\lp\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp}}\exp\lp-\frac{1}{2}\lp\vec{\tilde{F}}-\bm\rp^T\lp\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp^{-1}\lp\vec{\tilde{F}}-\bm\rp\rp\] Maximizing the likelihood is equivalent to minimizing the negative log-likelihood: \[-\log p\lp\vec{\tilde{F}}\rp=\frac{1}{2}\lp\vec{\tilde{F}}-\bm\rp^T\lp\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp^{-1}\lp\vec{\tilde{F}}-\bm\rp+\frac{1}{2}\log\det\lp\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}\rp+N\log2\pi\] Let $\mathcal{L}=-\log p\lp\vec{\tilde{F}}\rp$, $\vec{y}=\vec{\tilde{F}}-\bm$, $\vec{K}_y=\vec{K}+\vec{\Sigma}_{\tilde{\vec{f}}}$, and $\varphi$ be \mbox{a parameter of $\vec{m}$:} \[\frac{\partial\mathcal{L}}{\partial\varphi}=\frac{1}{2}\lb\lp\frac{\partial\vec{y}^T}{\partial\varphi}\rp\vec{K}_y^{-1}\vec{y}+\vec{y}^T\vec{K}_y\lp\frac{\partial\vec{y}}{\partial\varphi}\rp\rb=\frac{1}{2}\lp2\vec{y}^T\vec{K}_y\lp\frac{\partial\vec{y}}{\partial\varphi}\rp\rp=-\vec{y}^T\vec{K}_y\lp\frac{\partial\bm}{\partial\varphi}\rp\] Let $\theta$ be a parameter of $\vec{k}$. Jacobi's formula \mbox{(Magnus \& Neudecker, 1999) gives:} \[\frac{\partial\mathcal{L}}{\partial\theta}=\vec{y}^T\lp\frac{\partial\vec{K}_y^{-1}}{\partial\theta}\rp\vec{y}+\frac{\partial|\vec{K}_y|}{\partial\theta}=-\frac{1}{2}\vec{y}^T\vec{K}^{-1}\lp\frac{\partial\vec{K}_y}{\partial\theta}\rp\vec{K}^{-1}\vec{y}+\frac{1}{2}\text{tr}\lp\vec{K}_y^{-1}\lp\frac{\partial\vec{K}_y}{\partial\theta}\rp\rp\] Partials $\frac{\partial\vec{m}}{\partial\varphi}$ and $\frac{\partial\vec{K}_y}{\partial\theta}$ depend on the specific kernels. Our choices were: \[\vec{m}(\cdot)=\vec{c}\qquad\vec{k}(\cdot,\cdot')=\sigma^2\exp\lp-z\rp\vec{\Sigma}\qquad z=\frac{\|\cdot-\cdot'\|^2}{2\lambda^2}\qquad\vec{\Sigma}=\begin{bmatrix}\sigma_{uu}&\sigma_{uv}\\\sigma_{vu}&\sigma_{vv}\end{bmatrix}\] We set $\vec{\Sigma}=\vec{I}$, but let's consider $\frac{\partial\vec{K}}{\partial\sigma_{uu}}$ \mbox{(the others are analogous). We have:} \[\frac{\partial\vec{m}}{\partial\vec{c}}=\vec{1}\qquad\qquad\frac{\partial\vec{k}}{\partial\lambda}=\sigma^2\exp\lp-z\rp\Sigma\lp\frac{\|\cdot-\cdot'\|^2}{\lambda^3}\rp=\frac{\vec{k}\|\cdot-\cdot'\|^2}{\lambda^3}\] \[\frac{\partial\vec{k}}{\partial\sigma}=2\sigma\exp\lp-z\rp\vec{\Sigma}=\frac{2\vec{k}}{\sigma}\qquad\qquad\frac{\partial\vec{k}}{\partial\sigma_{uu}}=\sigma^2\exp\lp-z\rp\begin{bmatrix}1&0\\0&0\end{bmatrix}\] These extend naturally to the partials below: \[\frac{\partial\bm}{\partial\vec{c}}=\vec{1}\qquad\qquad\frac{\partial\vec{K}}{\partial\lambda}=K\odot\frac{\|\cdot-\cdot'\|^2}{\lambda^3}\qquad\qquad\frac{\partial\vec{K}}{\partial\sigma}=\frac{2\vec{K}}{\sigma}\] \[\frac{\partial\vec{K}}{\partial\sigma_{uu}}=\sigma^2\exp\lp-z\rp\begin{bmatrix}\vec{1}_{n\times n}&\vec{O}_{n\times n}\\\vec{O}_{n\times n}&\vec{O}_{n\times n}\end{bmatrix}\] These derivatives compose the gradient vector at each descent step.
\end{document}

