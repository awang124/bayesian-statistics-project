\documentclass[notitlepage, 12pt]{report}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{setspace}
\usepackage[letterpaper, portrait, margin=1.5in]{geometry}

\title{\vspace{-1.5cm} Spatial Gaussian Process Regression for \\ Probabilistic Optical Flow Estimation}
\author{Andy Wang}
\date{May 2025}
\begin{document}
\maketitle

\newcommand{\lb}{\left[}
\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}
\newcommand{\rb}{\right]}
\newcommand{\mb}{\mathbf}
\newcommand{\ex}{\mathbb{E}}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\bm}{\boldsymbol{\mu}}
\newcommand{\be}{\boldsymbol{\eta}}
\renewcommand{\thesection}{\arabic{section}}

\begin{abstract}
Classical computer vision methods have long been used to estimate optical flow fields for image sequences. These methods produce deterministic point estimates, failing to capture uncertainty inherent in optical flow estimation. I discuss spatial Gaussian process regression (GPR) to specify probability distributions for latent optical flow fields conditioned on observing these estimates. I expose the theory of GPR, describe procedures to fit these distributions to observed data, and apply spatial GPR to estimate optical flows of a real image sequence.
\end{abstract}

\section{Introduction}
\hspace{\parindent} Optical flow fields are an invaluable interdisciplinary tool, used throughout computer vision, robotics, human-computer interaction, medical imaging, and scientific research to perform object detection, motion segmentation, object tracking, action recognition, video stabilization, video compression, image registration, etc. A motion field is a 3-dimensional vector field that describes the velocity of objects moving in space: an optical flow field is a projection of such a motion field onto a 2-dimensional plane, viewed alternatively as a 2D vector field expressing the motion occurring between two frames in a sequence of images at each pixel. Optical flow estimation is the inverse problem of estimating this field given only a pair of images. \\

As a natural consequence of optical flows' widespread use, accurate optical flow estimation has been an active subject of research for decades. A simple correlation-based method found, for each window in the first image, that window in the second image most similar to it, then used the difference between these window locations as a flow vector (Micheli, 2025). An early gradient-based method assumed the image as a function of a parametric curve describing an object's motion is constant, then used the minimizer of its first derivative as a flow (Micheli, 2025). Two seminal methods were (Horn \& Schunck, 1981) and (Lucas \& Kanade, 1981). The former introduced global smoothness by regularizating flow gradients to prevent drastic variation in flow estimates of nearby pixels: the latter introduced local smoothness by computing flows for all pixels in a neighborhood via least-squares estimation. Modern deep learning approaches include FlowNet (Dosovitskiy et al., 2015) and Recurrent All-Pairs Field Transforms (RAFT) (Teed \& Deng, 2020), which employ a convolutional neural network and recurrent neural network, respectively. \\

These methods are all deterministic in nature, producing point (exact) optical flow estimates without any measure of confidence or uncertainty in their predictions. Uncertainty is inherent in optical flow estimation, due to image noise, brightness changes, lack of texture, object occlusion, the aperture problem, and multiple incompatible motions in localized regions. Furthermore, quantification of confidence is paramount in applications where erroneous estimates might have catastropic consequences, such as autonomous navigation, computer-integrated surgery, and real-time surveillance. As such, significant efforts have been made to make optical flow estimation probabilistic in nature, by specifying distributions of optical flows: some of thse efforts have employed simple maximum-a-posteriori estimation (Simoncelli et al., 1991), Markov random fields (Roy \& Govindu, 2000), Markov chain Monte Carlo (Sun et al., 2017), and mean field approximation (Wannenwetsch et al., 2017). \\

I propose a nonparametric Bayesian approach that uses existing deterministic methods to inform probabilistic modeling. I assume a joint Gaussian prior over the latent true optical flow field, making the flows random vectors following a 2-dimensional Gaussian process determined by a chosen mean and covariance kernel. I use the Horn-Schunck and Lucas-Kanade methods to generate observed flow fields, then compute the posterior distribution of the latent field conditioned on each observed field. The kernel parameters, and subsequently the latent mean and covariance, are optimized by maximizing the likelihood of the observations via gradient descent. After optimization, the posterior mean and covariance provide each pixel's optical flow estimate and associated uncertainty, respectively. I discuss how this approach may be extended to the interpolation of optical flows, via a posterior distribution conditioned on an incomplete set of observations. I apply this method to optical flow computation of the Yosemite sequence (Barron et al., 1994), a notable optical flow benchmark dataset consisting of 15 synthetic images, generated to resemble those captured by a drone flying over Yosemite National Park. Finally, I discuss my results and potential directions for improvement.

\section{Methodology}
\subsection{Spatial Gaussian Processes}
A 2-dimensional \textit{spatial Gaussian process} is a collection of random variables: \[\{\mb{f}(\mb{x})\}_{\mb{x}\in\mathcal{D}}=\left\{\begin{bmatrix}u(\mb{x})\\v(\mb{x})\end{bmatrix}\right\}_{\mb{x}\in\mathcal{D}}\sim\mathcal{GP}(\mb{m}(\mb{x}),\mb{k}(\mb{x},\mb{x}'))\] such that every finite subset follows the joint Gaussian distribution: \[\begin{bmatrix}\mb{f}(\mb{x}_1)\\\vdots\\\mb{f}(\mb{x}_n)\end{bmatrix}\sim\mathcal{N}\lp\begin{bmatrix}\mb{m}(\mb{x}_1)\\\ddots\\\mb{m}(\mb{x}_n)\end{bmatrix},\begin{bmatrix}\mb{k}(\mb{x}_1,\mb{x}_1)&\cdots&\mb{k}(\mb{x}_1,\mb{x}_n)\\\vdots&\ddots&\vdots\\\mb{k}(\mb{x}_n,\mb{x}_1)&\cdots&\mb{k}(\mb{x}_n,\mb{x}_n)\end{bmatrix}\rp\] determined by mean kernel $\mb{m}$ and covariance kernel $\mb{k}$, the choice of which encodes the prior over the process (Álvarez et al., 2012). Common $\mb{m}$ include zero, constant, and affine kernels: \[\mb{m}(\mb{x})=\mb{0}\qquad\mb{m}(\mb{x})=\mb{c}\qquad\mb{m}(\mb{x})=\mb{A}\mb{x}+\mb{b}\] Common $\mb{k}$ include Gaussian ($L^2$) and Laplace ($L^1$) kernels: \[\mb{k}_{L^2}(\mb{x},\mb{x}')=\sigma^2\exp\lp-\frac{\|\mb{x}-\mb{x}'\|^2}{2\lambda^2}\rp\mb{\Sigma}\qquad\mb{k}_{L^1}(\mb{x},\mb{x}')=\sigma^2\exp\lp-\frac{\|\mb{x}-\mb{x}'\|}{\lambda}\rp\mb{\Sigma}\] I chose a constant mean and Gaussian covariance, encoding a constant drift and loss of correlation as flows grow farther apart. Note that while kernel formulations are user-chosen, kernel parameters are optimized via likelihood maximization. An equivalent joint distribution formulation neatly separating horizontal, vertical, and cross covariances is: \[\mb{F}\coloneq\begin{bmatrix}\mb{U}(\mb{X})\\\mb{V}(\mb{X})\end{bmatrix}\sim\mathcal{N}\lp\bm\coloneq\begin{bmatrix}\bm_u\\\bm_v\end{bmatrix},\mb{K}\coloneq\begin{bmatrix}\mb{K}_{uu}&\mb{K}_{uv}\\\mb{K}_{vu}&\mb{K}_{vv}\end{bmatrix}\rp\] $\mb{X}=\{\mb{x}_i\}_{i=1}^N$, $\mb{U}(\mb{X})_i=u(\mb{x}_i)$, $\mb{V}(\mb{X})_i=v(\mb{x}_i)$, $(\bm_u)_i=\mb{m}(\mb{x}_i)_1$, \mbox{$(\bm_v)_i=\mb{m}(\mb{x}_i)_2$} \\\\ $\mb{\tilde{k}}=\mb{k}(\mb{x}_i,\mb{x}_j)\implies$ $(\mb{K}_{uu})_{ij}=\mb{\tilde{k}}_{11}$, $(\mb{K}_{uv})_{ij}=\mb{\tilde{k}}_{12}$, $(\mb{K}_{vu})_{ij}=\mb{\tilde{k}}_{21}$, $(\mb{K}_{vu})_{ij}=\mb{\tilde{k}}_{22}$. \\\\ In this work, $\mb{F}\in\mathbb{R}^{2N}$, where $N$ is the number of pixels, is the true flow field to be estimated, with concatenated horizontal and vertical components.

\subsection{Classical Observations}
We work with two sets of optical flow observations, computed via the Horn-Schunck and Lucas-Kanade methods. Each is structured as a vector: \[\mb{\tilde{F}}=\begin{bmatrix}\tilde{u}(\mb{x}_1)&\cdots&\tilde{u}(\mb{x}_N)&\tilde{v}(\mb{x}_1)&\cdots&\tilde{v}(\mb{x}_N)\end{bmatrix}^T\in\mathbb{R}^{2N}\] where $N$ is the number of image pixels, $\{\mb{x}_i\}_{i=1}^N$ are pixel indices, and $\{\tilde{u}(\mb{x}_i)\}_{i=1}^N$ and $\{\tilde{v}(\mb{x}_i)\}_{i=1}^N$ are horizontal and vertical flow components, respectively. For mathematical details, see (Horn \& Schunck, 1981), (Lucas \& Kanade, 1981).

\subsection{Gaussian Process Regression}
We assume each Horn-Schunck/Lucas-Kanade observed flow is its corresponding true flow plus some independent additive Gaussian noise: \[\mb{\tilde{f}}(\mb{x})=\mb{f}(\mb{x})+\mb{\be}(\mb{x}),\quad\mb{\be}(\mb{x})\sim\mathcal{N}\lp\mb{0},\sigma_{\be}^2\mb{I}\rp\] We want to determine marginal distribution $\mb{\tilde{F}}|\mb{X}$ via: \[p(\mb{\tilde{F}}|\mb{X})=\int p(\mb{\tilde{F}}|\mb{F},\mb{X})p(\mb{F}|\mb{X})\,d\mb{F}\] The convolution of two Gaussians is Gaussian (Rasmussen \& Williams, 2006). \[\mb{\tilde{F}}|\mb{F},\mb{X}\equiv\mb{\tilde{F}}|\mb{F}\sim\mathcal{N}\lp\mb{F},\sigma_{\be}^2\mb{I}\rp\qquad\mb{F}|\mb{X}\sim\mathcal{N}\lp\bm,\mb{K}\rp\] We use these to compute the mean and variance of $\mb{\tilde{F}}|\mb{X}$: \[\ex[\mb{\tilde{F}}|\mb{X}]=\ex[\ex[\mb{\tilde{F}}|\mb{F}]|\mb{X}]=\ex[\mb{F}|\mb{X}]=\bm\] \[\var(\mb{\tilde{F}}|\mb{X})=\var(\ex[\mb{\tilde{F}}|\mb{F}]|\mb{X})+\ex[\var(\mb{\tilde{F}}|\mb{F})|\mb{X}]=\var(\mb{F}|\mb{X})+\ex[\sigma_{\be}^2\mb{I}|\mb{X}]=\mb{K}+\sigma_{\be}^2\mb{I}\] Thus $\mb{\tilde{F}}|\mb{X}\sim\mathcal{N}\lp\bm,\mb{K}+\sigma_{\be}^2\mb{I}\rp$. See Appendix A for \mbox{a more through proof. Also:} \[\cov(\mb{F},\mb{\tilde{F}})=\cov(\mb{F},\mb{F})+\cov(\mb{F},\sigma_{\be}^2\mb{I})=\mb{K}+\mb{0}=\mb{K}\] All this allows us to formulate the joint distribution of $(\mb{F},\mb{\tilde{F}})$: \[\begin{bmatrix}\mb{F}\\\mb{\tilde{F}}\end{bmatrix}\sim\mathcal{N}\lp\begin{bmatrix}\bm\\\bm\end{bmatrix},\begin{bmatrix}\mb{K}&\mb{K}\\\mb{K}&\mb{K}+\sigma_{\be}^2\mb{I}\end{bmatrix}\rp\] Finally, we derive the posterior distribution $\mb{F}|\mb{\tilde{F}}$ \mbox{(Rasmussen \& Williams, 2006):} \[\mb{F}|\mb{\tilde{F}}\sim\mathcal{N}\lp\bm+\mb{K}\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp^{-1}\lp\mb{\tilde{F}}-\bm\rp,\mb{K}-\mb{K}\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp^{-1}\mb{K}\rp\] See Appendix B for a full derivation. This posterior gives us our updated optical flow estimates and uncertainties. It is computed after kernel parameters (e.g. $\sigma,\lambda,\sigma_{\be}$) are optimized via likelihood maximization (Rasmussen \& Williams, 2006) using gradient descent. See Appendix C for mathematical details. Gaussian process regression is also used to make predictions. We form the joint distribution of incomplete observations $\mb{F}_{\mb{X}}$, and latent flows $\mb{F}_*$ (different pixel locations) using an analogous mathematical formulation: \[\begin{bmatrix}\mb{F}_*\\\mb{F}_X\end{bmatrix}\sim\mathcal{N}\lp\begin{bmatrix}\bm_*\\\bm_X\end{bmatrix},\begin{bmatrix}\mb{K}_{**}&\mb{K}_{*X}\\\mb{K}_{X*}&\mb{K}_{XX}\end{bmatrix}\rp\] Predictions for $\mb{F}_*$ are then given by posterior distribution $\mb{F}_*|\mb{F}_{\mb{X}}$: \[\footnotesize \mb{F}_*|\mb{F}_X\sim\mathcal{N}\lp\bm_*+\mb{K}_{*X}\lp\mb{K}_{XX}+\sigma_{\be}^2\mb{I}\rp^{-1}\lp\mb{F}_X-\bm_X\rp,\mb{K}_{**}-\mb{K}_{*X}\lp\mb{K}_{XX}+\sigma_{\be}^2\mb{I}\rp^{-1}\mb{K}_{X*}\rp\] 

\section{Implementation}
The images below are the first two images of the Yosemite sequence. Motion is minute, perhaps most visible at the images' lower left corners: \begin{center} \includegraphics[totalheight=5.5cm]{img/16-1.png} \end{center}

The images are $252\times316$ RGBA images, which I grayscaled and contrast enhanced via histogram equalization. Spatial and temporal derivatives were computed using Sobel filters and simple differencing, respectively. Horn-Schunck computation was performed using 100 iterations with smoothness parameter $\lambda=300$: Lucas-Kanade computation was performed using $35\times35$ windows. GPR was performed separately for the two flow fields: in each case, a Gaussian process with constant mean and Gaussian covariance kernels was fit to a downsampled $25\times31$ field, due to memory constraints, using the Adam optimizer (Kingma \& Ba, 2014) to perform gradient descent with learning rate $\eta=0.1$. All code was written in Python.

\section{Results}
We illustrate the results of Horn-Schunck-based GPR. Downsampled for visual clarity, the figure below depicts each pixel's prior estimate (left) and posterior mean (right). Notice that GPR flows have uniformly smoothed the classical flows, denoising the image and incorporating correlation of neighboring pixels. Consider how the loss of information at the images' lower left edge, yielding motion in all directions, has been corrected: the clouds, however, with motion distinct from that of the cliffs, have been smoothed erroneously.
\begin{center} \includegraphics[totalheight=5.5cm]{img/16-2.png} \end{center}
Each pixel's posterior covariance is a $2\times2$ matrix: its diagonal entries quantify GPR's uncertainty (variance) in that individual prediction, decomposed into components. The figure below displays these entries, again downsampled:
\begin{center} \includegraphics[totalheight=5.5cm]{img/16-3.png} \end{center}
\newpage \noindent
Below are the analogous figures for the results of Lucas-Kanade-based GPR:
\begin{center} \includegraphics[totalheight=5.5cm]{img/17-2.png} \includegraphics[totalheight=5.5cm]{img/17-3.png} \end{center}

\section{Discussion}
\hspace{\parindent} My novel optical flow estimation methodology combining Horn-Schunck and Lucas-Kanade optical flow computation with spatial Gaussian process regression proved effective in enforcing spatial smoothness between optical flows and quantifying prediction uncertainties. This methodology was built upon a few key assumptions, however, that must be addressed. The first is an accurate set of prior optical flow estimates. While this was achieved for the Yosemite sequence, a well-known benchmark dataset, this may not always be possible for more complex real-world image sequences. Classical methods work best in highly textured regions (e.g. object corners and edges), but may perform suboptimally in regions of low texture or contrast (Micheli, 2025). Additionally, we must also have identically distributed additive Gaussian noise for each pixel: in practice, image noise may not be Gaussian, and may depend on pixel locations (e.g. more noise at object boundaries).

This project showed that spatial Gaussian process regression has potential for enhancement and confidence quantification of optical flow estimation. There are a myriad of directions in which to extend this work, and I list the most interesting of these below. \\

\noindent In legitimizing this work as a research endeavor:
\begin{itemize}
\item Validate the potential of the GPR approach by experimenting with complex real-world datasets (e.g. KITTI, MPI Sintel, FlyingChairs).
\item Evaluate pre-GPR and post-GPR flows against ground truth vectors when possible to quantify GPR's efficacy (e.g. using endpoint error). % \[\text{Endpoint Error}=\frac{1}{N}\sum_{i=1}^N\sqrt{(u_i-\hat{u}_i)^2+(v_i-\hat{v}_I)^2}\]
\end{itemize}

\noindent In interpreting results:
\begin{itemize}
\item Identify which hyperparameters suit which image sequences, through theoretical exploration or empirical comparison. For example, when is Gaussian or Laplace covariance preferred over the other?
\item Analyze posterior covariance off-diagonal elements, encoding correlation between individual pixels' horizontal and vertical motions.
\end{itemize}

\noindent In improving results:
\begin{itemize}
\item Obtain better observations, likely via deep learning (e.g. FlowNet, RAFT).
\item Perform semantic/instance segmentation, then perform GPR on each segment independently, to capture different motions in localized regions.
\item Establish more complex priors (e.g. non-uniform/non-Gaussian noise).
\item Exploit efficient representations of sparse flows for reduced computation time, allowing minimal image downsampling in running GPR.
\end{itemize}

\noindent In extending existing theory:
\begin{itemize}
\item Determine if a 3-dimensional Gaussian process can model sequences of optical flow fields, capturing spatial and temporal flow correlations.
\end{itemize}

Optical flow fields are a vastly valuable tool across industries and applications. This work has demonstrated the potential for probabilistic optical flow computation through Gaussian processes, to offer a small contribution to the continual improvement of this great aid for innovation and insight.

\newpage
\begin{spacing}{0.7}
\section*{References}
Álvarez, M. A., Rosasco, L., \& Lawrence, N. D. (2012). Kernels for vector-valued functions: A review. \textit{Foundations and Trends in Machine Learning, 4}(3), 195-266. \\\\
Barron, J. L., Fleet, D. J., \& Beauchemin, S. S. (1994). Performance of Optical Flow Techniques. \textit{International Journal of Computer Vision, 12}(1), 43-77. \\\\
Dosovitskiy A., Fischer P., Ilg E., Häusser P., Hazirbaş C., \& Golkov V. (2015). FlowNet: Learning Optical Flow with Convolutional Networks. \textit{Proceedings of the IEEE International Conference on Computer Vision}, 2758-2766. \\\\
Horn, B. K. P., \& Schunck, B. G. (1981). Determining Optical Flow. \textit{Artificial Intelligence, 17}(1-3), 185-203. \\\\
Kingma, D. P., \& Ba, J. (2014). Adam: A Method for Stochastic Optimization. \textit{3rd International Conference for Learning Representations}. \\\\
Lucas, B. D., \& Kanade, T. (1981). An Iterative Image Registration Technique with an Application to Stereo Vision. \textit{7th International Joint Conference on Artificial Intelligence, 2}, 674-679. \\\\
Magnus, J. R., \& Neudecker, H. (1999). Matrix Differential Calculus with Applications in Statistics and Econometrics. Wiley. \\\\
Micheli, M. (2025). Mathematical Image Analysis [Unpublished lecture notes]. Johns Hopkins University. \\\\
Rasmussen, C. E., \& Williams, C. K. I. (2006). Gaussian Process for Machine Learning. MIT Press. \\\\
Roy, S., \& Govindu, V. (2000). MRF Solutions for Probabilistic Optical Flow Formulations. \textit{Proceedings 15th International Conference on Pattern Recognition, 3}, 1041-1047. \\\\
Simoncelli, E. P., Adelson, E. H., \& Heeger, D. J. (1991). Probability Distributions of Optical Flow. \textit{CVPR, 91}, 310-315. \\\\
Sun, J., Quevedo, F. J., \& Bollt, E. (2018). Bayesian optical flow with uncertainty quantification. \textit{Inverse Problems 34}(10), 105008. \\\\
Teed, Z., \& Deng, J. (2020). RAFT: Recurrent All-Pairs Field Transforms for Optical Flow. \textit{Computer Vision--ECCV 2020: 16th European Conference, 2}(16), 402-419. \\\\
Wannenwetsch, A. S., Keuper, M., \& Roth, S. (2017). ProbFlow: Joint Optical Flow and Uncertainty Estimation. \textit{Proceedings of the IEEE International Conference on Computer Vision}, 1173-1182. \\\\
Zhang, F. (2005). The Schur complement and its applications. Springer Science \& Business Media.
\end{spacing}

\section*{Appendix A}
Here I prove $\mb{\tilde{F}}|\mb{F}\sim\mathcal{N}\lp\mb{F},\sigma_{\be}^2\mb{I}\rp$ and $\mb{F}|\mb{X}\sim\mathcal{N}\lp\bm,\mb{K}\rp$ implies $\mb{\tilde{F}}|\mb{X}\sim\mathcal{N}\lp\bm,\mb{K}\rp$: \[\footnotesize \begin{aligned} p\lp\mb{\tilde{F}}|\mb{X}\rp &= \int p\lp\mb{\tilde{F}}|\mb{F},\mb{X}\rp p\lp\mb{F}|\mb{X}\rp\,d\mb{F} \\& \propto\int\exp\lp-\frac{1}{2}\lp\mb{\tilde{F}}-\mb{F}\rp^T\frac{1}{\sigma_{\be}^2}\mb{I}\lp\mb{\tilde{F}}-\mb{F}\rp\rp\exp\lp-\frac{1}{2}\lp\mb{F}-\bm\rp^T\mb{K}^{-1}\lp\mb{F}-\bm\rp\rp\,d\mb{F} \\&= \int\exp\lp-\frac{1}{2}\lb\frac{1}{\sigma_{\be}^2}\mb{F}^T\mb{F}-\frac{2}{\sigma_{\be}^2}\mb{\tilde{F}}^T\mb{F}+\frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}^T\mb{\tilde{F}}+\mb{F}^T\mb{K}^{-1}\mb{F}-2\bm^T\mb{K}^{-1}\mb{F}+\bm^T\mb{K}^{-1}\bm\rb\rp\,d\mb{F} \end{aligned}\] We are integrating over $\mb{F}$ and briefly group all terms not involving $\mb{F}$ \mbox{into a constant:} \[\footnotesize p\lp\mb{\tilde{F}}|\mb{X}\rp\propto\int\exp\lp-\frac{1}{2}\lb\mb{F}^T\lp\frac{1}{\sigma_{\be}^2}\mb{I}+\mb{K}^{-1}\rp\mb{F}-2\lp\frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}^T+\bm^T\mb{K}^{-1}\rp\mb{F}+\text{const.}\rb\rp\,d\mb{F}\] Let $\mb{A}\coloneq\frac{1}{\sigma_{\be}^2}\mb{I}+\mb{K}^{-1},\mb{b}^T\coloneq\frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}^T-\bm^T\mb{K}^{-1}$. We complete the square: \[\footnotesize \mb{F}^T\mb{AF}-2\mb{b}^T\mb{F}=\lp\mb{F}-\mb{A}^{-1}\mb{b}\rp^T\mb{A}\lp\mb{F}-\mb{A}^{-1}\mb{b}\rp-\mb{b}^T\mb{A}^{-1}\mb{b}\] This yields: \[\footnotesize p\lp\mb{\tilde{F}}|\mb{X}\rp\propto\exp\lp-\frac{1}{2}\lp\mb{F}-\mb{A}^{-1}\mb{b}\rp^T\mb{A}\lp\mb{F}-\mb{A}^{-1}\mb{b}\rp\rp\exp\lp\frac{1}{2}\lb\mb{b}^T\mb{A}^{-1}\mb{b}-\frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}^T\mb{\tilde{F}}-\bm^T\mb{K}^{-1}\bm\rb\rp\,d\mb{F}\] The first term integrates to the inverse of the Gaussian normalizing factor. Meanwhile, the second term does not depend on $\mb{F}$. Reincorporating the initial normalizing factor, we obtain: \[\footnotesize p\lp\mb{\tilde{F}}|\mb{X}\rp=\frac{1}{\lp2\pi\sigma_{\be}\rp^{2N}\sqrt{\det\lp\mb{K}\rp}}\sqrt{\lp2\pi\rp^{2N}\det\lp\mb{A}^{-1}\rp}\exp\lp\frac{1}{2}\lb\mb{b}^T\mb{A}^{-1}\mb{b}-\frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}^T\mb{\tilde{F}}-\bm^T\mb{K}^{-1}\bm\rb\rp\] We use the intermediate result: \[\footnotesize \mb{A}=\mb{K}^{-1}+\frac{1}{\sigma_{\be}^2}\mb{I}=\mb{K}^{-1}\lp\mb{I}+\frac{1}{\sigma_{\be}^2}\mb{K}\rp=\frac{1}{\sigma_{\be}^2}\mb{K}^{-1}\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp\] to compute $\det\lp\mb{A}^{-1}\rp$: \[\footnotesize \det\lp\mb{A}\rp^{-\frac{1}{2}}=\lp\frac{1}{\sigma_{\be}^{2n}}\rp^{-\frac{1}{2}}\det\lp\mb{K}^{-1}\rp^{-\frac{1}{2}}\det\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp^{-\frac{1}{2}}=\sigma_{\be}^n\det\lp\mb{K}\rp^{\frac{1}{2}}\det\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp^{-\frac{1}{2}}\] Substituting this into the above expression yields: \[\footnotesize p\lp\mb{\tilde{F}}|\mb{X}\rp=\frac{1}{\sqrt{2\pi\det\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp}}\exp\lp\frac{1}{2}\lb\mb{b}^T\mb{A}^{-1}\mb{b}-\frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}^T\mb{\tilde{F}}-\bm^T\mb{K}^{-1}\bm\rb\rp\] We express $\mb{b}^T\mb{A}^{-1}\mb{b}$ in terms of $\mb{\tilde{F}},\bm,\mb{K}$. Let $\mb{K}_{\sigma}\coloneq\mb{K}+\sigma_{\be}^2\mb{I}$: \[\begin{aligned} \mb{b}^T\mb{A}^{-1}\mb{b} &= \lp\frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}-\mb{K}^{-1}\bm\rp^T\lp\mb{I}+\frac{1}{\sigma_{\be}^2}\mb{K}\rp^{-1}\mb{K}\lp\frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}+\mb{K}^{-1}\bm\rp \\&= \sigma_{\be}^2\lp\frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}+\mb{K}^{-1}\bm\rp^T\mb{K}_{\sigma}^{-1}\mb{K}\lp\frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}+\mb{K}^{-1}\bm\rp \\&= \sigma_{\be}^2\lb\frac{1}{\sigma_{\be}^4}\mb{\tilde{F}}^T\mb{K}_{\sigma}^{-1}\mb{K}\mb{\tilde{F}}+\frac{2}{\sigma_{\be}^2}\mb{\tilde{F}}^T\mb{K}_{\sigma}^{-1}\bm+\bm^T\mb{K}^{-1}\mb{K}_{\sigma}^{-1}\bm\rb \\&= \frac{1}{\sigma_{\be}^2}\mb{\tilde{F}}^T\mb{\tilde{F}}-\mb{\tilde{F}}^T\mb{K}_{\sigma}^{-1}\mb{\tilde{F}}+2\mb{\tilde{F}}^T\mb{K}_{\sigma}^{-1}\bm+\sigma_{\be}^2\bm^T\mb{K}^{-1}\mb{K}_{\sigma}^{-1}\bm \end{aligned}\] Using this, the previous expression becomes: \[\begin{aligned} p\lp\mb{\tilde{F}}|\mb{X}\rp & \propto\exp\lp\frac{1}{2}\lb-\mb{\tilde{F}}^T\mb{K}_{\sigma}^{-1}\mb{\tilde{F}}+2\mb{\tilde{F}}\mb{K}_{\sigma}^{-1}\bm+\bm^T\lp\sigma_{\eta}^2\mb{K}^{-1}\mb{K}_{\sigma}^{-1}-\mb{K}^{-1}\rp\bm\rb\rp \\&= \exp\lp\frac{1}{2}\lb-\mb{\tilde{F}}^T\mb{K}_{\sigma}^{-1}\mb{\tilde{F}}+2\mb{\tilde{F}}^T\mb{K}_{\sigma}^{-1}\bm+\text{const.}\rb\rp \end{aligned}\] We again complete the square: \[-\mb{\tilde{F}}^T\mb{K}_{\sigma}^{-1}\mb{\tilde{F}}+2\mb{\tilde{F}}\mb{K}_{\sigma}^{-1}\bm=-\lp\mb{\tilde{F}}-\bm\rp^T\mb{K}_{\sigma}^{-1}\lp\mb{\tilde{F}}-\bm\rp+\bm^T\mb{K}_{\sigma}^{-1}\bm\] This yields: \[\footnotesize p\lp\mb{\tilde{F}}|\mb{X}\rp\propto\exp\lp-\frac{1}{2}\lp\mb{\tilde{F}}-\bm\rp^T\mb{K}_{\sigma}^{-1}\lp\mb{\tilde{F}}-\bm\rp\rp\exp\lp\frac{1}{2}\bm^T\lb\mb{K}_{\sigma}^{-1}+\sigma_{\eta}^2\mb{K}^{-1}\mb{K}_{\sigma}^{-1}-\mb{K}^{-1}\rb\bm\rp\] The middle term in the second exponent is: \[\mb{K}_{\sigma}^{-1}+\sigma_{\eta}^2\mb{K}^{-1}\mb{K}_{\sigma}^{-1}-\mb{K}^{-1}=\lp\mb{I}+\sigma_{\be}^2\mb{K}^{-1}\rp\mb{K}_{\sigma}^{-1}-\mb{K}^{-1}=\mb{K}^{-1}\mb{K}_{\sigma}\mb{K}_{\sigma}^{-1}-\mb{K}^{-1}=\mb{O}\] Thus we achieve the final expression for the density: \[p\lp\mb{\tilde{F}}|\mb{X}\rp=\frac{1}{\sqrt{2\pi\det\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp}}\exp\lp-\frac{1}{2}\lp\mb{\tilde{F}}-\bm\rp^T\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp^{-1}\lp\mb{\tilde{F}}-\bm\rp\rp\] and subsequently the target distribution: \[\mb{\tilde{F}}|\mb{X}\sim\mathcal{N}\lp\bm,\mb{K}+\sigma_{\be}^2\mb{I}\rp\]

\newpage
\section*{Appendix B}
Here I prove that if $\mb{X}_1,\mb{X}_2$ are multivariate normal, i.e.: \[\begin{bmatrix}\mb{X}_1\\\mb{X}_2\end{bmatrix}\sim\mathcal{N}\lp\begin{bmatrix}\bm_1\\\bm_2\end{bmatrix},\begin{bmatrix}\mb{\Sigma}_{11}&\mb{\Sigma}_{12}\\\mb{\Sigma}_{21}&\mb{\Sigma}_{22}\end{bmatrix}\rp\] then the conditional distribution of $\mb{X}_1|\mb{X}_2$ is: \[\mb{X}_1|\mb{X}_2\sim\mathcal{N}\lp\bm_1+\mb{\Sigma}_{12}\mb{\Sigma}_{22}^{-1}\lp\mb{X}_2-\bm_2\rp,\mb{\Sigma}_{11}-\mb{\Sigma}_{12}\mb{\Sigma}_{22}^{-1}\mb{\Sigma}_{21}\rp\] Conditioning on $\mb{X}_2$ means $\mb{X}_2$ is fixed. Thus: \[p_{\mb{X}_1|\mb{X}_2}(\mb{x}_1|\mb{x}_2)=\frac{p_{\mb{X}_1,\mb{X}_2}(\mb{x}_1,\mb{x}_2)}{p_{\mb{X}_2}(\mb{x}_2)}\propto p_{\mb{X}_1,\mb{X}_2}(\mb{x}_1,\mb{x}_2)\] Let $\mb{\tilde{\Sigma}}=\mb{\Sigma}^{-1}$. This density is proportional to: \[\exp\lp-\frac{1}{2}\begin{bmatrix}\mb{x}_1-\bm_1\\\mb{x}_2-\bm_2\end{bmatrix}^T\begin{bmatrix}\mb{\tilde{\Sigma}}_{11}&\mb{\tilde{\Sigma}}_{12}\\\mb{\tilde{\Sigma}}_{21}&\mb{\tilde{\Sigma}}_{22}\end{bmatrix}\begin{bmatrix}\mb{x}_1-\bm_1\\\mb{x}_2-\bm_2\end{bmatrix}\rp\] Notice that $\mb{\tilde{\Sigma}}_{ij}$ equals $\lp\mb{\Sigma}^{-1}\rp_{ij}$ and not $\lp\mb{\Sigma}_{ij}\rp^{-1}$. \mbox{Let $Q$ be the exponent. We expand:} \[\small Q\propto\lp\mb{x}_1-\bm_1\rp^T\mb{\tilde{\Sigma}}_{11}\lp\mb{x}_1-\bm_1\rp+2\lp\mb{x}_1-\bm_1\rp^T\mb{\tilde{\Sigma}}_{12}\lp\mb{x}_2-\bm_2\rp+\lp\mb{x}_2-\bm_2\rp^T\mb{\tilde{\Sigma}}_{22}\lp\mb{x}_2-\bm_2\rp\] We expand further and ignore all constant terms (those not involving $\mb{x}_1$): \[Q=\mb{x}_1^T\mb{\tilde{\Sigma}}_{11}\mb{x}_1-2\mb{x}_1^T\mb{\tilde{\Sigma}}_{11}\bm_1+2\mb{x}_1^T\mb{\tilde{\Sigma}}_{12}\lp\mb{x}_2-\bm_2\rp+\text{const.}\] We complete the square by finding $\bm_*,\mb{\Sigma}_*$: \[\lp\mb{x}_1-\bm_*\rp^T\mb{\Sigma}_*^{-1}\lp\mb{x}_1-\bm_*\rp=\mb{x}_1^T\mb{\Sigma}_*^{-1}\mb{x}_1-2\mb{x}_1\mb{\Sigma}_*^{-1}\bm_*=Q\] The quadratic terms must equate: \[\mb{x}_1^T\mb{\Sigma}_*^{-1}\mb{x}_1=\mb{x}_1^T\mb{\tilde{\Sigma}}_{11}\mb{x}_1\implies\mb{\Sigma}_*^{-1}=\mb{\tilde{\Sigma}}_{11}\implies\mb{\Sigma}_*=\mb{\tilde{\Sigma}}_{11}^{-1}\] The linear terms must also equate: \[-2\mb{x}_1\mb{\Sigma}_*^{-1}\bm_*=-2\mb{x}_1^T\mb{\tilde{\Sigma}}_{11}\bm_1+2\mb{x}_1^T\mb{\tilde{\Sigma}}_{12}\lp\mb{x}_2-\bm_2\rp\] Substituting $\mb{\Sigma}_*^{-1}=\mb{\tilde{\Sigma}}_{11}$ and simplifying yields: \[\mb{\tilde{\Sigma}}_{11}\bm_*=\mb{\tilde{\Sigma}}_{11}\bm_1-\mb{\tilde{\Sigma}}_{12}\lp\mb{x}_2-\bm_2\rp\implies\bm_*=\bm_1-\mb{\tilde{\Sigma}}_{11}^{-1}\mb{\tilde{\Sigma}}_{12}\lp\mb{x}_2-\bm_2\rp\] Thus, we have: \[p_{\mb{X}_1|\mb{X}_2}\propto\exp\lp-\frac{1}{2}\lp\mb{x}_1-\bm_*\rp^T\mb{\Sigma}_*\lp\mb{x}_1-\bm_*\rp\rp\implies\mb{X}_1|\mb{X}_2\sim\mathcal{N}\lp\bm_*,\mb{\Sigma}_*\rp\] We have absorbed all constants into the final density's normalizing factor. \\\\ All that remains is to determine $\mb{\tilde{\Sigma}}_{11},\mb{\tilde{\Sigma}}_{12}$. Zhang (2005) gives: \[\mb{M}=\begin{bmatrix}\mb{A}&\mb{B}\\\mb{C}&\mb{D}\end{bmatrix}=\begin{bmatrix}\mb{I}&\mb{BD}^{-1}\\\mb{O}&\mb{I}\end{bmatrix}\begin{bmatrix}\mb{S}&\mb{O}\\\mb{O}&\mb{D}\end{bmatrix}\begin{bmatrix}\mb{I}&\mb{O}\\\mb{D}^{-1}\mb{C}&\mb{I}\end{bmatrix}\] where Schur complement $\mb{S}\coloneq\mb{A}-\mb{BD}^{-1}\mb{C}$. We invert: \[\begin{aligned} \mb{M}^{-1} &= \begin{bmatrix}\mb{I}&-\mb{BD}^{-1}\\\mb{O}&\mb{I}\end{bmatrix}\begin{bmatrix}\mb{S}^{-1}&\mb{O}\\\mb{O}&\mb{D}^{-1}\end{bmatrix}\begin{bmatrix}\mb{I}&\mb{O}\\-\mb{D}^{-1}\mb{C}&\mb{I}\end{bmatrix} \\&= \begin{bmatrix}\mb{S}^{-1}&-\mb{S}^{-1}\mb{BD}^{-1}\\-\mb{D}^{-1}\mb{CS}^{-1}&\mb{D}^{-1}+\mb{D}^{-1}\mb{CS}^{-1}\mb{BD}^{-1}\end{bmatrix} \end{aligned}\] For $\mb{M}=\mb{\Sigma}$, Schur complement $\mb{\Sigma}_{S}\coloneq\mb{\Sigma}_{11}-\mb{\Sigma}_{12}\mb{\Sigma}_{22}^{-1}\mb{\Sigma}_{21}$, and: \[\mb{\tilde{\Sigma}}_{11}=\lp\mb{\Sigma}^{-1}\rp_{11}=\mb{\Sigma}_S^{-1}\qquad\mb{\tilde{\Sigma}}_{12}=\lp\mb{\Sigma}^{-1}\rp_{12}=-\mb{\Sigma}_S^{-1}\mb{\Sigma}_{12}\mb{\Sigma}_{22}^{-1}\] From these we derive the intermediate result: \[\mb{\tilde{\Sigma}}_{11}^{-1}\mb{\tilde{\Sigma}}_{12}=\mb{\Sigma}_S\lp-\mb{\Sigma}_S^{-1}\mb{\Sigma}_{12}\mb{\Sigma}_{22}^{-1}\rp=\mb{\Sigma}_{12}\mb{\Sigma}_{22}^{-1}\] These results yield expressions for $\bm_*,\mb{\Sigma}_*$ in terms of $\bm,\mb{\Sigma}$: \[\bm_*=\bm_1-\mb{\tilde{\Sigma}}_{11}^{-1}\mb{\tilde{\Sigma}}_{12}\lp\mb{x}_2-\bm_2\rp=\bm_1+\mb{\Sigma}_{12}\mb{\Sigma}_{22}^{-1}\lp\mb{x}_2-\bm_2\rp\] \[\mb{\Sigma}_*=\mb{\tilde{\Sigma}}_{11}^{-1}=\mb{\Sigma}_S=\mb{\Sigma}_{11}-\mb{\Sigma}_{12}\mb{\Sigma}_{22}^{-1}\mb{\Sigma}_{21}\] We have fully derived conditional distribution $\mb{X}_1|\mb{X}_2$. Substituting the appropriate mean and covariance parameters from the formulations of Gaussian process regression into this general expression yields \mbox{the posterior distributions:} \[\mb{F}|\mb{\tilde{F}}\sim\mathcal{N}\lp\bm+\mb{K}\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp^{-1}\lp\mb{\tilde{F}}-\bm\rp,\mb{K}-\mb{K}\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp^{-1}\mb{K}\rp\] \[\mb{F}_*|\mb{F}_X\sim\mathcal{N}\lp\bm_*+\mb{K}_{*X}\lp\mb{K}_{XX}+\sigma_{\be}^2\mb{I}\rp^{-1}\lp\mb{F}_X-\bm_X\rp,\mb{K}_{**}-\mb{K}_{*X}\lp\mb{K}_{XX}+\sigma_{\be}^2\mb{I}\rp^{-1}\mb{K}_{X*}\rp\]

\section*{Appendix C}
Here I show how to maximize the marginal likelihood of $\mb{\tilde{F}}|\mb{X}\sim\mathcal{N}\lp\bm,\mb{K}+\sigma_{\be}^2\mb{I}\rp$: \[p\lp\mb{\tilde{F}}|\mb{X}\rp=\frac{1}{\sqrt{(2\pi)^{2N}\det\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp}}\exp\lp-\frac{1}{2}\lp\mb{\tilde{F}}-\bm\rp^T\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp^{-1}\lp\mb{\tilde{F}}-\bm\rp\rp\] Maximizing the likelihood is equivalent to minimizing the negative log-likelihood: \[-\log p\lp\mb{\tilde{F}}|\mb{X}\rp=\frac{1}{2}\lp\mb{\tilde{F}}-\bm\rp^T\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp^{-1}\lp\mb{\tilde{F}}-\bm\rp+\frac{1}{2}\log\det\lp\mb{K}+\sigma_{\be}^2\mb{I}\rp+N\log2\pi\] Let $\mathcal{L}=-\log p\lp\mb{\tilde{F}}|\mb{X}\rp$, $\mb{y}=\mb{\tilde{F}}-\bm$, $\mb{K}_y=\mb{K}+\sigma_{\be}^2\mb{I}$, and $\varphi$ be \mbox{a parameter of $\mb{m}$:} \[\frac{\partial\mathcal{L}}{\partial\varphi}=\frac{1}{2}\lb\lp\frac{\partial\mb{y}^T}{\partial\varphi}\rp\mb{K}_y^{-1}\mb{y}+\mb{y}^T\mb{K}_y\lp\frac{\partial\mb{y}}{\partial\varphi}\rp\rb=\frac{1}{2}\lp2\mb{y}^T\mb{K}_y\lp\frac{\partial\mb{y}}{\partial\varphi}\rp\rp=-\mb{y}^T\mb{K}_y\lp\frac{\partial\bm}{\partial\varphi}\rp\] Let $\theta$ be a parameter of $\mb{k}$. Jacobi's formula \mbox{(Magnus \& Neudecker, 1999) gives:} \[\frac{\partial\mathcal{L}}{\partial\theta}=\mb{y}^T\lp\frac{\partial\mb{K}_y^{-1}}{\partial\theta}\rp\mb{y}+\frac{\partial|\mb{K}_y|}{\partial\theta}=-\frac{1}{2}\mb{y}^T\mb{K}^{-1}\lp\frac{\partial\mb{K}_y}{\partial\theta}\rp\mb{K}^{-1}\mb{y}+\frac{1}{2}\text{tr}\lp\mb{K}_y^{-1}\lp\frac{\partial\mb{K}_y}{\partial\theta}\rp\rp\] Partials $\frac{\partial\mb{m}}{\partial\varphi}$ and $\frac{\partial\mb{K}_y}{\partial\theta}$ depend on the specific kernels. My choices were: \[\mb{m}(\mb{x})=\mb{c}\qquad\mb{k}(\mb{x},\mb{x}')=\sigma^2\exp\lp-z\rp\mb{\Sigma}\qquad z=\frac{\|\mb{x}-\mb{x}\|^2}{2\lambda^2}\qquad\mb{\Sigma}=\begin{bmatrix}\sigma_{uu}&\sigma_{uv}\\\sigma_{vu}&\sigma_{vv}\end{bmatrix}\] I set $\mb{\Sigma}=\mb{I}$, but let's consider $\frac{\partial\mb{K}}{\partial\sigma_{uu}}$ \mbox{(the others are analogous). We have:} \[\frac{\partial\mb{m}}{\partial\mb{c}}=\mb{1}\qquad\qquad\frac{\partial\mb{k}}{\partial\lambda}=\sigma^2\exp\lp-z\rp\Sigma\lp\frac{\|\mb{x}-\mb{x}\|^2}{\lambda^3}\rp=\frac{\mb{k}\|\mb{x}-\mb{x}\|^2}{\lambda^3}\] \[\frac{\partial\mb{k}}{\partial\sigma}=2\sigma\exp\lp-z\rp\mb{\Sigma}=\frac{2\mb{k}}{\sigma}\qquad\qquad\frac{\partial\mb{k}}{\partial\sigma_{uu}}=\sigma^2\exp\lp-z\rp\begin{bmatrix}1&0\\0&0\end{bmatrix}\] These extend naturally to the partials below. We also include $\frac{\partial\mb{K}}{\partial\sigma_{\be}}$: \[\frac{\partial\bm}{\partial\mb{c}}=\mb{1}\qquad\qquad\frac{\partial\mb{K}}{\partial\lambda}=K\odot\frac{\|\mb{x}-\mb{x}\|^2}{\lambda^3}\qquad\qquad\frac{\partial\mb{K}}{\partial\sigma}=\frac{2\mb{K}}{\sigma}\] \[\frac{\partial\mb{K}}{\partial\sigma_{uu}}=\sigma^2\exp\lp-z\rp\begin{bmatrix}\mb{1}_{n\times n}&\mb{O}_{n\times n}\\\mb{O}_{n\times n}&\mb{O}_{n\times n}\end{bmatrix}\qquad\qquad\frac{\partial\mb{K}}{\partial\sigma_{\be}}=2\sigma_{\eta}\mb{I}\] These derivatives compose the gradient vector at each descent step.
\end{document}

